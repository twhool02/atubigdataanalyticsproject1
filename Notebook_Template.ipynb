{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZ2JdBuDHdALyOGyjxtLCz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twhool02/atubigdataanalyticsproject1/blob/main/Notebook_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Notebook Title Here"
      ],
      "metadata": {
        "id": "tkZueMZDz3qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "A8ueI6JEzR6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map Google Drive"
      ],
      "metadata": {
        "id": "wIZMjLkbz-Z5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJMQBD7OzMi5",
        "outputId": "1ce91074-783d-4f47-a9c4-106ff05e08bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import shutil, os, subprocess\n",
        "\n",
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/dissertation')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log into HuggingFace Hub"
      ],
      "metadata": {
        "id": "rVHNqlQF0D-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required when quantizing models/data that are gated on HuggingFace and required for pushing models to HuggingFace\n",
        "!pip install --upgrade huggingface_hub\n",
        "\n",
        "import huggingface_hub\n",
        "\n",
        "print(f\"Hugging Face Version is: {huggingface_hub.__version__}\")"
      ],
      "metadata": {
        "id": "1BZtoDRFze_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed402905-2672-4f74-c64a-4ff9e45139c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.2)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "Installing collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.20.2\n",
            "    Uninstalling huggingface-hub-0.20.2:\n",
            "      Successfully uninstalled huggingface-hub-0.20.2\n",
            "Successfully installed huggingface_hub-0.20.3\n",
            "Hugging Face Version is: 0.20.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# using the HF_TOKEN secret, this has write permissions to Hugging Face\n",
        "hftoken = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "Zxe1lqJYVrSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Log into hugging face using the HF_TOKEN secrect\n",
        "login(hftoken, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "TxT3T40GV21k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Transformers and other libraries"
      ],
      "metadata": {
        "id": "p0SOco760QWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the development version of transformers\n",
        "# !pip install -q -U git+https://github.com/huggingface/transformers.git -q\n",
        "\n",
        "\n",
        "# The 'accelerate' library is a part of the Hugging Face ecosystem[^1^][1][^2^][2].\n",
        "# It enables the same PyTorch code to be run across any distributed configuration by adding just a few lines of code[^1^][1][^2^][2].\n",
        "# In short, it makes training and inference at scale simple, efficient, and adaptable[^1^][1][^2^][2].\n",
        "# It abstracts the boilerplate code related to multi-GPUs/TPU/fp16 and leaves the rest of your code unchanged[^2^][2].\n",
        "# This library is useful when you want to easily run your training scripts in a distributed environment without having to renounce full control over your training loop[^2^][2][^3^][3].\n",
        "# It is not a high-level framework above PyTorch, just a thin wrapper so you don't have to learn a new library[^2^][2][^3^][3].\n",
        "# !pip install -q -U git+https://github.com/huggingface/accelerate\n",
        "\n",
        "\n",
        "# Install latest available stable builds, upgrade if later version that the currently installed version is available\n",
        "!pip install -q -U transformers -q\n",
        "\n",
        "# Accelerate is a Python library created for PyTorch users. It abstracts the boilerplate code related to using multiple GPUs, TPUs,\n",
        "# and mixed precision (fp8, fp16, bf16), allowing you to run your raw PyTorch training script on any kind of device.\n",
        "# By adding a few lines to any standard PyTorch training script, you can now run on any kind of single or distributed\n",
        "# node setting (single CPU, single GPU, multi-GPUs, and TPUs) as well as with or without mixed precision.\n",
        "# It even handles the device placement for you\n",
        "!pip install -q -U accelerate -q\n",
        "\n",
        "\n",
        "# TensorFlow is an open-source software library for high-performance numerical computation3.\n",
        "# It’s used for machine learning and deep learning applications34. Its flexible architecture allows easy deployment\n",
        "# of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers\n",
        "# to mobile and edge devices3.\n",
        "# It was originally developed by researchers and engineers from the Google Brain team within Google’s AI organization\n",
        "!pip install -q -U tensorFlow\n",
        "\n",
        "\n",
        "# 'einops' is a Python library that provides flexible and powerful tensor operations for readable and reliable code.\n",
        "# It supports various frameworks such as numpy, pytorch, tensorflow, jax, and others.\n",
        "# It allows you to rearrange, reduce, and repeat elements in tensors according to specified patterns.\n",
        "# This makes it easier to manipulate tensors in a more readable and reliable way.\n",
        "!pip install -q -U einops\n",
        "\n",
        "# 'sentencepiece' is a Python library for unsupervised text tokenization.\n",
        "# It provides an API for encoding, decoding, and training of Sentencepiece models.\n",
        "# It's particularly useful for Neural Network-based text generation.\n",
        "!pip install sentencepiece -q\n",
        "\n",
        "# The 'bitsandbytes' library is a lightweight wrapper around CUDA custom functions,\n",
        "# particularly 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions[^1^][4][^2^][5].\n",
        "# It is used for tasks like 8-bit inference with HuggingFace Transformers, using 8-bit optimizers,\n",
        "# and replacing certain layers with 8-bit versions for improved performance[^1^][4][^2^][5].\n",
        "!pip install bitsandbytes -q\n",
        "\n",
        "# PEFT stands for Parameter-Efficient Fine-Tuning, developed by Hugging Face that aims to make fine-tuning large language models (LLMs) more efficient and memory-friendly.\n",
        "# Key features of PEFT:\n",
        "# Parameter-efficient fine-tuning: It allows you to fine-tune only a small portion of a large language model's parameters, reducing memory usage and training time significantly.\n",
        "# Adaptive embedding sharing: It dynamically determines which embeddings to share across different tasks, further optimizing memory usage.\n",
        "# Gradient checkpointing: It saves memory by storing only a subset of activations during backpropagation.\n",
        "# Compatibility with Transformers: It integrates seamlessly with the popular Transformers library, making it easy to use with various pre-trained language models.\n",
        "!pip install peft -q\n",
        "\n",
        "# trl is short for Transformers Reinforcement Learning, it's used for fine-tuning transformer models using Proximal Policy Optimization.\n",
        "!pip install trl -q\n",
        "\n",
        "# The 'xformers' library provides customizable and optimized building blocks for Transformers[^3^][1].\n",
        "# It is domain-agnostic and used by researchers in various fields like vision, NLP, etc[^3^][1].\n",
        "# The library contains bleeding-edge components that are not yet available in mainstream libraries like PyTorch[^3^][1].\n",
        "# It is built with efficiency in mind, containing its own CUDA kernels, but dispatches to other libraries when relevant[^3^][1].\n",
        "# !pip install xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjVKGesI0PwM",
        "outputId": "28bf7ed0-d9d9-410e-a5f7-c44c1e06022b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "4.37.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check library versions"
      ],
      "metadata": {
        "id": "Z8_EvaWmSA-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the version of transformers\n",
        "import transformers\n",
        "print(f\"version of transformers: {transformers.__version__}\")\n",
        "\n",
        "# print the version of the tensorflow library\n",
        "import accelerate\n",
        "print(f\"version of accelerate: {accelerate.__version__}\")\n",
        "\n",
        "# print the version of the tensorflow library\n",
        "import tensorflow as tf\n",
        "print(f\"version of tensorflow: {tf.__version__}\")"
      ],
      "metadata": {
        "id": "J5b9lIPTSCBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart the runtime"
      ],
      "metadata": {
        "id": "cBPTbEQvRoQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# restart the runtime\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "nzJHxSrdRr6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "0Ph2m1bFRxb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the torch library. PyTorch is an open source machine learning library based on the Torch library.\n",
        "# It's used for applications such as computer vision and natural language processing.\n",
        "import torch\n",
        "\n",
        "# Import the nn module from the torch library. nn stands for neural networks.\n",
        "# This module contains various building blocks for creating neural networks.\n",
        "import torch.nn as nn\n",
        "\n",
        "# Import specific classes from the transformers library. The transformers library is a state-of-the-art Natural Language Processing library for TensorFlow 2.0 and PyTorch.\n",
        "# AutoTokenizer: This class can automatically guess and download the correct tokenizer based on the model’s name you give it (like 'bert-base-uncased').\n",
        "# AutoConfig: This class can automatically guess and download the correct configuration based on the model’s name you give it (like 'bert-base-uncased').\n",
        "# AutoModelForCausalLM: This class can automatically guess and download a model for causal language modeling based on the model’s name you give it (like 'gpt2').\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "tHq6Xnki8Xml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch is a library for processing tensors. A tensor is a generalization of vectors and matrices to potentially higher dimensions.\n",
        "# bfloat16 is a 16-bit floating point representation method within the tensor library.\n",
        "# It is used for mixed precision training where some operations use the torch.float32 datatype and other operations use a lower precision floating point datatype like torch.bfloat16[^1^][1][^2^][2].\n",
        "from torch import bfloat16\n",
        "\n",
        "# transformers is a library that provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, etc.) for Natural Language Understanding (NLU) and Natural Language Generation (NLG).\n",
        "# The pipeline() function is a high-level, easy to use, API for doing inference over a variety of downstream-tasks, including Named Entity Recognition (NER), Masked Language Modeling (MLM), Sentiment Analysis, Feature Extraction and Question Answering[^3^][4][^4^][5].\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "qgUUTJOt2b2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libaries imported when inferring a model"
      ],
      "metadata": {
        "id": "CMA85KuhVqDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# os is a standard Python library that provides functions for interacting with the operating system.\n",
        "import os\n",
        "\n",
        "# torch is the main package of PyTorch, an open-source machine learning library for Python.\n",
        "import torch\n",
        "\n",
        "# load_dataset is a function from the datasets library by Hugging Face. It allows you to load and preprocess datasets for machine learning models.\n",
        "from datasets import load_dataset\n",
        "\n",
        "# The transformers library is a popular library for Natural Language Processing (NLP). It provides thousands of pre-trained models to perform tasks on texts such as classification, information extraction, summarization, translation, and more.\n",
        "from transformers import (\n",
        "    # AutoModelForCausalLM is a class in the transformers library. It represents a model for causal language modeling.\n",
        "    AutoModelForCausalLM,\n",
        "\n",
        "    # AutoTokenizer is a class in the transformers library. It is used for converting input data into a format that can be used by the model.\n",
        "    AutoTokenizer,\n",
        "\n",
        "    # BitsAndBytesConfig is a configuration class in the transformers library. It is used to configure a BitsAndBytes model.\n",
        "    BitsAndBytesConfig,\n",
        "\n",
        "    # HfArgumentParser is a class in the transformers library. It is used for parsing command-line arguments.\n",
        "    HfArgumentParser,\n",
        "\n",
        "    # TrainingArguments is a class in the transformers library. It defines the arguments used during training.\n",
        "    TrainingArguments,\n",
        "\n",
        "    # pipeline is a high-level function in the transformers library. It creates a pipeline that applies a model to some input data.\n",
        "    pipeline,\n",
        "\n",
        "    # logging is a module in the transformers library. It is used for logging events during training and evaluation.\n",
        "    logging,\n",
        ")\n",
        "\n",
        "# used for Parameter-Efficient Fine-Tuning\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "# trl is short for Transformers Reinforcement Learning. It is a Python library for fine-tuning transformer models using Proximal Policy Optimization.\n",
        "from trl import SFTTrainer\n"
      ],
      "metadata": {
        "id": "3tCUMzeWVpNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create cache directory for Hugging Face Models"
      ],
      "metadata": {
        "id": "E0iaAOGFWH8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the cache directory to a specific path in your Google Drive.\n",
        "# This is where Hugging Face models will be cached.\n",
        "cache_dir = \"/content/drive/MyDrive/huggingface_cache\"\n",
        "\n",
        "# The os.makedirs() method in Python is used to create directories recursively.\n",
        "# The exist_ok=True parameter prevents an error if the directory already exists.\n",
        "os.makedirs(cache_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "nYjfvdo3WLUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading"
      ],
      "metadata": {
        "id": "OJTfAcUpWThy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load a Pre-Trained Model"
      ],
      "metadata": {
        "id": "GD4xgh9yWXCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the name of the model to be used. This is a string that corresponds to a specific pre-trained model.\n",
        "# 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T' is the name of the model.\n",
        "# model_name = 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'\n",
        "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v0.3'\n",
        "\n",
        "# Load the pre-trained model using the AutoModelForCausalLM class from the transformers library.\n",
        "# The from_pretrained() method is used to load the model.\n",
        "# device_map='cpu' specifies that the model should be loaded onto the CPU. If you have a GPU available, you could change this to 'cuda' to use the GPU instead.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, # specifies which pre-trained model to load\n",
        "    trust_remote_code=True, # allows the execution of remote code. Be careful with this setting as it can be a security risk.\n",
        "    torch_dtype=torch.bfloat16, #sets the data type for the model's parameters to bfloat16.\n",
        "    offload_folder='offload', #specifies the folder where offloaded parameters will be stored.\n",
        "    cache_dir=cache_dir # sets the directory where the pre-trained model will be cached. This can help speed up future model loading times.\n",
        ")"
      ],
      "metadata": {
        "id": "vCJWTkQOWZS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load a Pre-Trained Model from a local directory"
      ],
      "metadata": {
        "id": "XbPODvomHrWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zephyr with BitsAndBytes Configuration\n",
        "\n",
        "# The AutoTokenizer class is used to load the tokenizer associated with the \"HuggingFaceH4/zephyr-7b-alpha\" model.\n",
        "# A tokenizer is responsible for preparing the inputs for a model. This includes converting input text into tokens, which are numerical representations that the model can understand.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
        "\n",
        "# The AutoModelForCausalLM class is used to load the \"HuggingFaceH4/zephyr-7b-alpha\" model with the specified quantization configuration.\n",
        "# AutoModelForCausalLM is a class that includes automatic model architecture detection. It's used for tasks that involve causal language modeling (predicting the next word in a sentence).\n",
        "# The 'quantization_config' parameter is set to 'bnb_config', which is the BitsAndBytesConfig instance we created earlier. This configures the model to use the specified quantization settings.\n",
        "# The 'device_map' parameter is set to 'auto', which means the model will automatically use the GPU if it's available, and fall back to CPU otherwise.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    cache_dir=cache_dir,\n",
        "    local_files_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "_Ge5HkI_HoQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a pipeline"
      ],
      "metadata": {
        "id": "AG42k-bxIYGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "# The pipeline function is a high-level, easy to use, API for doing inference over a variety of downstream-tasks, including text generation.\n",
        "# It abstracts away the underlying details and allows you to use models like BERT, GPT-2 or RoBERTa with a simple API.\n",
        "# The model and tokenizer are passed as arguments to the function.\n",
        "# The model is a pre-trained model and the tokenizer is used to convert the input text into a format that is understandable by the model.\n",
        "# The task argument specifies the task to be performed. In this case, it’s ‘text-generation’.\n",
        "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
      ],
      "metadata": {
        "id": "TBvpkZiiIczX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linux Commands\n",
        "\n"
      ],
      "metadata": {
        "id": "HguoNcT6WgwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a directory"
      ],
      "metadata": {
        "id": "l4Z-jVCZYHCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# The os.makedirs() method in Python is used to create directories recursively.\n",
        "# The exist_ok=True parameter prevents an error if the directory already exists.\n",
        "os.makedirs('/content/drive/MyDrive/new_dirextory', exist_ok=True)"
      ],
      "metadata": {
        "id": "5T5bAuBBYGZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Change directory"
      ],
      "metadata": {
        "id": "96sdrnfjSNJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# change directory\n",
        "os.chdir(cache_dir)"
      ],
      "metadata": {
        "id": "g_wR6UE3SMfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get current working directory and list files"
      ],
      "metadata": {
        "id": "8z0jpfI5RfNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# get current working dirctory and list files\n",
        "print(f\"current directory is: {os.getcwd()}\\n\")\n",
        "# print(os.listdir('.'))\n",
        "\n",
        "# Get a list of all files and directories in the current directory\n",
        "files = glob.glob('./*')\n",
        "\n",
        "# Create a list of tuples, each containing the name of the file/directory and its last modification time\n",
        "files_with_times = [(file, os.path.getmtime(file)) for file in files]\n",
        "\n",
        "# Sort the list by the modification time (the second element of each tuple)\n",
        "files_with_times.sort(key=lambda x: x[1])\n",
        "\n",
        "# Print the sorted list\n",
        "print(\"Files in current directory:\")\n",
        "for file, mtime in files_with_times:\n",
        "    print(f'{file}: {mtime}')"
      ],
      "metadata": {
        "id": "9Bqmo1TZWpEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download a model from HF"
      ],
      "metadata": {
        "id": "F359zF94Xo-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "model_id=\"twhoool02/TinyLlama-1.1B-Chat-v0.3-GGUF\"\n",
        "\n",
        "snapshot_download(repo_id=model_id, local_dir=cache_dir, local_dir_use_symlinks=False, revision=\"main\")"
      ],
      "metadata": {
        "id": "Q9FRFgs0Xt5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push the model to Hugging Face"
      ],
      "metadata": {
        "id": "Y1FRRjyXYb8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the HfApi class from the huggingface_hub module\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Initialize the HfApi class\n",
        "api = HfApi()\n",
        "\n",
        "# set model name\n",
        "quant_name = model_name.split('/')[-1] + \"-GGUF\"\n",
        "print(f\"Model name is {quant_name}\")\n",
        "\n",
        "# Define the ID of the repository where the file will be uploaded\n",
        "# The repository is located under the user 'twhooly02' and named after the quant_name variable\n",
        "repo_id = \"twhoool02/\" + quant_name\n",
        "print(f\"Repository ID is {repo_id}\")\n",
        "\n",
        "# Create model repo\n",
        "api.create_repo(repo_id=repo_id)"
      ],
      "metadata": {
        "id": "b878Yn-oYeyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory from where model files will be uploaded\n",
        "base_path = \"./models\"\n",
        "\n",
        "#Array of local file paths you want to upload\n",
        "local_file_paths = [\n",
        "    base_path + \"/tokenizer_config.json\",\n",
        "    base_path + \"/tokenizer.model\",\n",
        "    base_path + \"/tokenizer.json\",\n",
        "    base_path + \"/special_tokens_map.json\",\n",
        "    base_path + \"/ggml-vocab-llama.gguf\",\n",
        "    base_path + \"/\" + f'{model_name_pure}.{quant_type}.gguf',\n",
        "]"
      ],
      "metadata": {
        "id": "8UTYogeXYjyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loop through each file and upload it\n",
        "for local_file_path in local_file_paths:\n",
        "    # Extract the file name from the local file path\n",
        "    file_name = local_file_path.split(\"/\")[-1]\n",
        "\n",
        "    # Specify the path where you want the file to be uploaded in the repository\n",
        "    path_in_repo = file_name # Using the file name directly adjust if needed\n",
        "\n",
        "    # Use the upload_file method of the HfApi class to upload the file\n",
        "    # The method takes the local file path, the path in the repository, the repository ID, and the repository type as arguments\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=local_file_path,\n",
        "        path_in_repo=path_in_repo,\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\"  # The type of the repository is 'model', could also be \"dataset\" or \"space\"\n",
        "    )\n",
        "\n",
        "    print(f\"Uploaded {file_name} to {repo_id}\")"
      ],
      "metadata": {
        "id": "dQJ0RJqsYkPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Model Cards"
      ],
      "metadata": {
        "id": "dhCR33S9YqjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Model cards from template\n",
        "\n",
        "This is the preferred option"
      ],
      "metadata": {
        "id": "XliKAA4MYtCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import model card libraries\n",
        "from huggingface_hub import ModelCard, ModelCardData\n",
        "\n",
        "# Import the date class from the datetime module\n",
        "from datetime import date\n",
        "\n",
        "# The datasets the model was trained on\n",
        "datasets = [\"cerebras/SlimPajama-672B\",\"bigcode/starcoderdata\",\"OpenAssistant/oasst_top1_2023-08-25\"]\n",
        "\n",
        "# Define the metadata for the model card\n",
        "card_data = ModelCardData(\n",
        "    base_model='TinyLlama/TinyLlama-1.1B-Chat-v0.3',\n",
        "    language='en',  # The language the model was trained on\n",
        "    license='apache-2.0',  # The license for the model\n",
        "    library=['Transformers','GGUF'],  # The library used to train the model\n",
        "    model_name=quant_name,  # The name of the model\n",
        "    model_type='tinyllama',\n",
        "    tags=[\"GGUF\", \"tinyllama\"],  # Tags for the model\n",
        "    datasets=datasets\n",
        ")\n",
        "\n",
        "# Define a description for the model\n",
        "model_description = \"This model is a quantized version of the TinyLlama/TinyLlama-1.1B-Chat-v0.3 model. \\\n",
        "    The model was quantized using GGUF.\"\n",
        "\n",
        "# Create a model card from the template\n",
        "card = ModelCard.from_template(\n",
        "    card_data,  # The model card data defined earlier\n",
        "    model_id=quant_name,  # The ID of the model\n",
        "    model_description=model_description,  # The description of the model\n",
        "    date=date.today(),  # The date the model card was created\n",
        "    developers=\"Ted Whooley\"  # The developers of the model\n",
        ")\n",
        "\n",
        "# Print the model card\n",
        "print(card)"
      ],
      "metadata": {
        "id": "QTks7iAZYwZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add custom model card"
      ],
      "metadata": {
        "id": "sn5bd8LIY-PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add custom model card\n",
        "from huggingface_hub import ModelCard, ModelCardData\n",
        "\n",
        "datasets = ['cerebras/SlimPajama-672B','bigcode/starcoderdata','OpenAssistant/oasst_top1_2023-08-25']\n",
        "\n",
        "card_data = ModelCardData(language='en', license='apache-2.0', datasets=datasets)\n",
        "\n",
        "example_template_var = 'nateraw'\n",
        "content = f\"\"\"\n",
        "---\n",
        "{ card_data.to_yaml() }\n",
        "---\n",
        "\n",
        "# TinyLlama-1.1B-Chat-v0.3-GGUF\n",
        "\n",
        "This model is a quantized version of the TinyLlama/TinyLlama-1.1B-Chat-v0.3 model. The model was quantized using GGUF.\n",
        "\n",
        "The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**.\n",
        "\"\"\"\n",
        "\n",
        "card = ModelCard(content)\n",
        "print(card)"
      ],
      "metadata": {
        "id": "cSaA7WRJZAv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload the model card"
      ],
      "metadata": {
        "id": "IWB8Xls7Y3nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    card.validate()\n",
        "    card.save('README.md')\n",
        "    card.push_to_hub(repo_id=repo_id)\n",
        "except:\n",
        "    raise ValueError(\"model card info is invalid. please check.\")"
      ],
      "metadata": {
        "id": "TeYmIrKeY0Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Inference"
      ],
      "metadata": {
        "id": "n2Il1_qh6Wmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create prompt\n",
        "\n",
        "Note: These prompts may only work on the model - zephyr-7b-beta which I was using when following [Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)](https://https://www.maartengrootendorst.com/blog/quantization/) as in this case the chat template is saved in the underlying tokenizer"
      ],
      "metadata": {
        "id": "mhHeFQtQ6aoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the tokenizer's chat template to format each message\n",
        "# See https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "\n",
        "# 'messages' is a list of dictionaries. Each dictionary represents a message in the conversation.\n",
        "# Each message has a 'role' (either 'system' or 'user') and 'content' (the text of the message).\n",
        "# The 'system' role is typically used for instructions that guide the model's behavior.\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Tell me a story about a man from Ireland\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# The 'apply_chat_template' function formats the messages for the model.\n",
        "prompt = pipe.tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False, # 'tokenize=False' means the messages will not be tokenized right now.\n",
        "    add_generation_prompt=True # 'add_generation_prompt=True' adds a special prompt token at the end to signal the model to start generating a response.\n",
        ")"
      ],
      "metadata": {
        "id": "dGHf5wfx6esw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pass Prompt to LLM"
      ],
      "metadata": {
        "id": "XOPYgs506lku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'pipe' function is called with several arguments:\n",
        "outputs = pipe(\n",
        "    prompt, # 'prompt' is the input text that the model will respond to.\n",
        "    max_new_tokens=256, # 'max_new_tokens=256' limits the length of the generated text to 256 tokens.\n",
        "    do_sample=True, # 'do_sample=True' means the model will generate text by sampling from its output distribution.\n",
        "    temperature=0.1, # controls the randomness of the sampling process. Lower values (like 0.1) make the output more deterministic, while higher values make it more random.\n",
        "    top_p=0.95 # 'top_p=0.95' implements nucleus sampling, where the model only considers the smallest set of tokens whose cumulative probability exceeds 0.95.\n",
        ")\n",
        "\n",
        "# The 'outputs' variable contains the generated text.\n",
        "# 'outputs[0][\"generated_text\"]' extracts the generated text from the first (and in this case, only) output.\n",
        "# The generated text is then printed to the console.\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "2BWP2UcE6ils"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup"
      ],
      "metadata": {
        "id": "LTqTmeejZLhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete directories\n",
        "\n",
        "Have removed optiont to delete the directory in the below code it is too dangerous"
      ],
      "metadata": {
        "id": "e8R6E6koZNn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Specify the directory you want to delete\n",
        "directory = cache_dir\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(directory):\n",
        "    # Use shutil.rmtree to delete the directory\n",
        "    # shutil.rmtree(directory)\n",
        "    print(f\"The directory {directory} has been deleted.\")\n",
        "else:\n",
        "    print(\"The directory does not exist.\")\n"
      ],
      "metadata": {
        "id": "b4GnCoMXZSg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete Repo\n",
        "\n",
        "Have commented this out will uncomment as needed"
      ],
      "metadata": {
        "id": "OUtqVq5BZTfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # delete the repo that was created with this notebook\n",
        "# from huggingface_hub import delete_repo\n",
        "\n",
        "# delete_repo(repo_id=repo_id, repo_type=\"mo"
      ],
      "metadata": {
        "id": "2qvSeYROZW8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Troubleshooting"
      ],
      "metadata": {
        "id": "9AmERSRmnTPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check memory footprint"
      ],
      "metadata": {
        "id": "W_FjhqVanbFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU"
      ],
      "metadata": {
        "id": "WZO0XjhEnb-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "metadata": {
        "id": "9zV3tmBeneEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check processes running"
      ],
      "metadata": {
        "id": "81lsUEZ_nh2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "process_name_substring = 'python'\n",
        "\n",
        "result = subprocess.run(['fuser', '/dev/nvidia0', '-v'], stdout = subprocess.PIPE)\n",
        "\n",
        "process_ids = [int(i) for i in str(result.stdout).split(' ') if i.isdigit()]\n",
        "\n",
        "for process_id in process_ids:\n",
        "    pid_info = subprocess.run(['ps', '-p', str(process_id)], stdout = subprocess.PIPE)\n",
        "    print(pid_info.stdout)"
      ],
      "metadata": {
        "id": "HZSGNWDfnj7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get memory stats"
      ],
      "metadata": {
        "id": "4AcPq2d-o8VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'torch.cuda.memory_stats()' function returns a dictionary containing detailed CUDA memory stats.\n",
        "# This includes information about the amount of memory allocated and reserved on the GPU\n",
        "torch.cuda.memory_stats()"
      ],
      "metadata": {
        "id": "WIWIw96go7wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Empty the CUDA cache"
      ],
      "metadata": {
        "id": "DHq945TCpb3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'torch.cuda.empty_cache()' function releases all unoccupied cached memory\n",
        "# currently held by the caching allocator so that those can be used in other\n",
        "# GPU application and visible in nvidia-smi. It does not release the GPU\n",
        "# memory used by tensors. So if you are using GPU tensors, this will not\n",
        "# free any used GPU memory.\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "mDSZjd-mpfxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tricks"
      ],
      "metadata": {
        "id": "KAAX-dYYpzOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU compatibility with bfloat16"
      ],
      "metadata": {
        "id": "dy1PZSDip11t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "WYGB5ph5p8mf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}